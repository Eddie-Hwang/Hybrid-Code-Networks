{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './data/GoogleNews-vectors-negative300.bin'\n",
    "word2vec = KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "# model.wv['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 R_cuisine korean\n",
      "1 R_location seoul\n",
      "2 R_price cheap\n",
      "3 R_rating 1\n",
      "4 R_phone resto_seoul_cheap_korean_1stars_phone\n",
      "5 R_address resto_seoul_cheap_korean_1stars_address\n",
      "6 R_number four\n"
     ]
    }
   ],
   "source": [
    "entities = {}\n",
    "with open('dialog-bAbI-tasks/dialog-babi-kb-all.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    for l in lines:\n",
    "        wds = l.rstrip().split(' ')[2].split('\\t')\n",
    "        slot_type = wds[0] # ex) R_price\n",
    "        slot_val = wds[1] # ex) cheap\n",
    "        if slot_type not in entities:\n",
    "            entities[slot_type] = []\n",
    "        if slot_val not in entities[slot_type]:\n",
    "            entities[slot_type].append(slot_val)\n",
    "for idx, (ent_name, ent_vals) in enumerate(entities.items()):\n",
    "    print(idx, ent_name, ent_vals[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: for people two moderate range are looking for\n",
      "context: [0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def update_context(context, sentence):\n",
    "    for idx, (ent_key, ent_vals) in enumerate(entities.items()):\n",
    "        for w in sentence:\n",
    "            if w in ent_vals:\n",
    "#                 print('idx', idx, w)\n",
    "                context[idx] = 1\n",
    "\n",
    "#test\n",
    "t_context = [0] * len(entities.keys())\n",
    "t_sentence = 'for people two moderate range are looking for'\n",
    "update_context(t_context, t_sentence)\n",
    "print('sentence:', t_sentence)\n",
    "print('context:', t_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bow(sentence, vocab, w2i):\n",
    "    bow = [0] * len(vocab)\n",
    "    for word in sentence.split(' '):\n",
    "        if word in w2i:\n",
    "            bow[w2i[word]] += 1\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fpath, entities, vocab, system_acts):\n",
    "    data = []\n",
    "    with open(fpath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        x, y, c = [], [], []\n",
    "        context = [0] * len(entities.keys())\n",
    "        for idx, l in enumerate(lines):\n",
    "            l = l.rstrip()\n",
    "            if l == '':\n",
    "                data.append((x, y, c))\n",
    "                # reset\n",
    "                x, y, c = [], [], []\n",
    "                context = [0] * len(entities.keys())\n",
    "            else:\n",
    "                ls = l.split(\"\\t\")\n",
    "                t_u = ls[0].split(' ', 1)\n",
    "                turn = t_u[0]\n",
    "                uttr = t_u[1].split(' ')\n",
    "                update_context(context, uttr)\n",
    "                sys_act = SILENT\n",
    "                if len(ls) == 2: # includes user and system utterance\n",
    "                    for w in uttr:\n",
    "                        if w not in vocab: vocab.append(w)\n",
    "\n",
    "                    sys_act = ls[1]\n",
    "                    sys_act = re.sub(r'resto_\\S+', '', sys_act)\n",
    "                    if sys_act.startswith('api_call'): sys_act = 'api_call'\n",
    "                    if sys_act not in system_acts: system_acts.append(sys_act)\n",
    "                else:\n",
    "                    continue # TODO\n",
    "                        \n",
    "                x.append(uttr)\n",
    "                y.append(sys_act)\n",
    "                c.append(copy.copy((context)))\n",
    "    vocab = sorted(vocab)\n",
    "    return data, vocab, system_acts\n",
    "\n",
    "# create training dataset\n",
    "SILENT = '<SILENT>'\n",
    "system_acts = [SILENT]\n",
    "vocab = []\n",
    "fpath_train = 'dialog-bAbI-tasks/dialog-babi-task5-full-dialogs-trn.txt'\n",
    "fpath_test = 'dialog-bAbI-tasks/dialog-babi-task5-full-dialogs-tst-OOV.txt'\n",
    "train_data, vocab, system_acts = load_data(fpath_train, entities, vocab, system_acts)\n",
    "test_data, vocab, system_acts = load_data(fpath_test, entities, vocab, system_acts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max turn: 27\n",
      "action size: 16\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "# train, train_dlg, vocabs_train = load('dialog-bAbI-tasks/dialog-babi-task5-full-dialogs-trn.txt')\n",
    "max_turn_train = max([len(d[0]) for d in train_data])\n",
    "max_turn_test = max([len(d[0]) for d in test_data])\n",
    "max_turn = max(max_turn_train, max_turn_test)\n",
    "print('max turn:', max_turn)\n",
    "w2i = dict((w, i) for i, w in enumerate(vocab))\n",
    "i2w = dict((i, w) for i, w in enumerate(vocab))\n",
    "act2i = dict((act, i) for i, act in enumerate(system_acts))\n",
    "\n",
    "print('action size:', len(system_acts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<SILENT>': 0,\n",
       " 'any preference on a type of cuisine': 15,\n",
       " 'api_call': 7,\n",
       " 'great let me do the reservation': 11,\n",
       " 'hello what can i help you with today': 1,\n",
       " 'here it is ': 12,\n",
       " 'how many people would be in your party': 4,\n",
       " \"i'm on it\": 2,\n",
       " 'is there anything i can help you with': 13,\n",
       " 'ok let me look into some options for you': 6,\n",
       " 'sure is there anything else to update': 8,\n",
       " 'sure let me find an other option for you': 10,\n",
       " 'what do you think of this option: ': 9,\n",
       " 'where should it be': 3,\n",
       " 'which price range are looking for': 5,\n",
       " \"you're welcome\": 14}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def to_var(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "class WordEmbedding(nn.Module):\n",
    "    '''\n",
    "    In : (N, sentence_len)\n",
    "    Out: (N, sentence_len, embd_size)\n",
    "    '''\n",
    "    def __init__(self, vocab_size, embd_size, pre_embd_w=None, is_train_embd=False):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embd_size)\n",
    "        if pre_embd_w is not None:\n",
    "            self.embedding.weight = nn.Parameter(pre_embd_w, requires_grad=is_train_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class HybridCodeNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size, hidden_size, action_size):\n",
    "        super(HybridCodeNetwork, self).__init__()\n",
    "        self.embd_size = embd_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = WordEmbedding(vocab_size, embd_size)\n",
    "        self.lstm = nn.LSTM(307, hidden_size, batch_first=True) # TODO input dim\n",
    "        self.fc = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "#     def forward(self, uttr, context, act_mask, bow, last_act):\n",
    "    def forward(self, uttr, context):\n",
    "        # uttr: (bs, dialog_len, sentence_len)\n",
    "        # uttr: (bs, dialog_len, context_dim)\n",
    "        bs = uttr.size(0)\n",
    "        dlg_len = uttr.size(1)\n",
    "        sent_len = uttr.size(2)\n",
    "        \n",
    "        embd = self.embedding(uttr.view(bs, -1)) # (bs, dialog_len*sentence_len, embd)\n",
    "        embd = embd.view(bs, dlg_len, sent_len, -1) # (bs, dialog_len, sentence_len, embd)\n",
    "        embd = torch.mean(embd, 2) # (bs, dialog_len, embd)\n",
    "        x = torch.cat((embd, context), 2) # (bs, dialog_len, embd+context_dim)\n",
    "        x, (h, c) = self.lstm(x) # (bs, seq, hid), ((1, bs, hid), (1, bs, hid))\n",
    "        y = F.log_softmax(self.fc(x), -1) # (bs, seq, action_size)\n",
    "        return y\n",
    "\n",
    "embd_size = 300\n",
    "hidden_size = 100\n",
    "print(len(system_acts))\n",
    "model = HybridCodeNetwork(len(vocab), embd_size, hidden_size, len(system_acts))\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.22652828693389893\n",
      "loss 0.21633586287498474\n",
      "loss 0.21057692170143127\n",
      "loss 0.21129053831100464\n",
      "loss 0.2321590930223465\n",
      "loss 0.21111074090003967\n",
      "loss 0.19333162903785706\n",
      "loss 0.21138924360275269\n",
      "loss 0.21803469955921173\n",
      "loss 0.20133042335510254\n",
      "loss 0.2107846587896347\n",
      "loss 0.22052684426307678\n",
      "loss 0.19224828481674194\n",
      "loss 0.1939406543970108\n",
      "loss 0.18736137449741364\n",
      "loss 0.19156664609909058\n",
      "loss 0.19662989675998688\n",
      "loss 0.18234926462173462\n",
      "loss 0.17528751492500305\n",
      "loss 0.1813841015100479\n",
      "loss 0.19426864385604858\n",
      "loss 0.178115576505661\n",
      "loss 0.18112820386886597\n",
      "loss 0.16078197956085205\n",
      "loss 0.17916178703308105\n",
      "loss 0.1937027871608734\n",
      "loss 0.1767042875289917\n",
      "loss 0.17284417152404785\n",
      "loss 0.16177503764629364\n",
      "loss 0.17099489271640778\n",
      "loss 0.16867317259311676\n",
      "loss 0.15026849508285522\n",
      "loss 0.1678142100572586\n",
      "loss 0.16510629653930664\n",
      "loss 0.1562018245458603\n",
      "loss 0.15048418939113617\n",
      "loss 0.15733806788921356\n",
      "loss 0.1669473946094513\n",
      "loss 0.1580592542886734\n",
      "loss 0.16015255451202393\n",
      "loss 0.14651454985141754\n",
      "loss 0.14747390151023865\n",
      "loss 0.15220880508422852\n",
      "loss 0.135410875082016\n",
      "loss 0.15930241346359253\n",
      "loss 0.14278292655944824\n",
      "loss 0.1509237140417099\n",
      "loss 0.1339578479528427\n",
      "loss 0.14649271965026855\n",
      "loss 0.1445925086736679\n",
      "loss 0.14731451869010925\n",
      "loss 0.14420491456985474\n",
      "loss 0.12475549429655075\n",
      "loss 0.1360892355442047\n",
      "loss 0.12453950196504593\n",
      "loss 0.14501126110553741\n",
      "loss 0.12266489863395691\n",
      "loss 0.12929773330688477\n",
      "loss 0.12294081598520279\n",
      "loss 0.13027168810367584\n",
      "loss 0.12363991141319275\n",
      "loss 0.13338907063007355\n",
      "loss 0.12562550604343414\n",
      "loss 0.11833372712135315\n",
      "loss 0.12238570302724838\n",
      "loss 0.12425587326288223\n",
      "loss 0.11137844622135162\n",
      "loss 0.11878145486116409\n",
      "loss 0.11525461822748184\n",
      "loss 0.132930725812912\n",
      "loss 0.11129303276538849\n",
      "loss 0.12208656966686249\n",
      "loss 0.10297510027885437\n",
      "loss 0.11400981992483139\n",
      "loss 0.11188548803329468\n",
      "loss 0.1047181487083435\n",
      "loss 0.11372598260641098\n",
      "loss 0.11395814269781113\n",
      "loss 0.10767122358083725\n",
      "loss 0.09502968192100525\n",
      "loss 0.10282604396343231\n",
      "loss 0.11069368571043015\n",
      "loss 0.10424823313951492\n",
      "loss 0.10187939554452896\n",
      "loss 0.10213574022054672\n",
      "loss 0.10329513251781464\n",
      "loss 0.10249124467372894\n",
      "loss 0.10461422801017761\n",
      "loss 0.08986295759677887\n",
      "loss 0.09127878397703171\n",
      "loss 0.08432009816169739\n",
      "loss 0.09042036533355713\n",
      "loss 0.09452228993177414\n",
      "loss 0.10019513964653015\n",
      "loss 0.08999929577112198\n",
      "loss 0.08186269551515579\n",
      "loss 0.09207237511873245\n",
      "loss 0.09391586482524872\n",
      "loss 0.08769788593053818\n",
      "loss 0.08913075178861618\n",
      "loss 0.0874304249882698\n",
      "loss 0.08545702695846558\n",
      "loss 0.08392193168401718\n",
      "loss 0.0872742310166359\n",
      "loss 0.07874827086925507\n",
      "loss 0.08410539478063583\n",
      "loss 0.08599565178155899\n",
      "loss 0.07441332936286926\n",
      "loss 0.08454623818397522\n",
      "loss 0.07663483917713165\n",
      "loss 0.07685311138629913\n",
      "loss 0.07576335966587067\n",
      "loss 0.08165770024061203\n",
      "loss 0.07447357475757599\n",
      "loss 0.07434292882680893\n",
      "loss 0.07775069028139114\n",
      "loss 0.07694030553102493\n",
      "loss 0.07479782402515411\n",
      "loss 0.0768972709774971\n",
      "loss 0.07227513939142227\n",
      "loss 0.06709115952253342\n",
      "loss 0.06877532601356506\n",
      "loss 0.07309278100728989\n",
      "loss 0.07197289168834686\n",
      "loss 0.0694655254483223\n",
      "loss 0.06398000568151474\n",
      "loss 0.06967198103666306\n",
      "loss 0.07381779700517654\n",
      "loss 0.06634960323572159\n",
      "loss 0.06694985181093216\n",
      "loss 0.06347042322158813\n",
      "loss 0.06653979420661926\n",
      "loss 0.06757321953773499\n",
      "loss 0.05859538912773132\n",
      "loss 0.07010063529014587\n",
      "loss 0.06094750761985779\n",
      "loss 0.06404712051153183\n",
      "loss 0.06030270457267761\n",
      "loss 0.06257838755846024\n",
      "loss 0.06499750167131424\n",
      "loss 0.05984357371926308\n",
      "loss 0.062089886516332626\n",
      "loss 0.053773872554302216\n",
      "loss 0.06210114806890488\n",
      "loss 0.06071770563721657\n",
      "loss 0.05702288821339607\n",
      "loss 0.05520332604646683\n",
      "loss 0.055884603410959244\n",
      "loss 0.05466839671134949\n",
      "loss 0.057313110679388046\n"
     ]
    }
   ],
   "source": [
    "def add_padding(data, seq_len):\n",
    "    pad_len = max(0, seq_len - len(data))\n",
    "    data += [0] * pad_len\n",
    "    data = data[:seq_len]\n",
    "    return data\n",
    "\n",
    "def make_word_vector(uttrs_list, w2i, dialog_maxlen, uttr_maxlen):\n",
    "    dialog_list = []\n",
    "    for uttrs in uttrs_list:\n",
    "        dialog = []\n",
    "        for sentence in uttrs:\n",
    "            sent_vec = [w2i[w] for w in sentence]\n",
    "            sent_vec = add_padding(sent_vec, uttr_maxlen)\n",
    "            dialog.append(sent_vec)\n",
    "        for _ in range(dialog_maxlen - len(dialog)):\n",
    "            dialog.append([0] * uttr_maxlen)\n",
    "        dialog = torch.LongTensor(dialog[:dialog_maxlen])\n",
    "        dialog_list.append(dialog)\n",
    "    return to_var(torch.stack(dialog_list, 0))\n",
    "    \n",
    "def train(model, data, optimizer, n_epochs=10, batch_size=64):\n",
    "    for epoch in range(n_epochs):\n",
    "        random.shuffle(data)\n",
    "        data = copy.copy(data)\n",
    "        for i in range(0, len(data)-batch_size, batch_size):\n",
    "            batch = data[i:i+batch_size]\n",
    "            uttrs_list = [d[0] for d in batch]\n",
    "            dialog_maxlen = max([len(uttrs) for uttrs in uttrs_list])\n",
    "            uttr_maxlen = max([len(u) for uttrs in uttrs_list for u in uttrs])\n",
    "#             print('dialog_maxlen', dialog_maxlen, ', uttr_maxlen', uttr_maxlen)\n",
    "            uttr_var = make_word_vector(uttrs_list, w2i, dialog_maxlen, uttr_maxlen)\n",
    "            batch_labels = [d[1] for d in batch]\n",
    "            labels_var = []\n",
    "            for labels in batch_labels:\n",
    "                vec_labels = [act2i[l] for l in labels]\n",
    "                pad_len = dialog_maxlen - len(labels)\n",
    "#                 print('b vec_labels', len(vec_labels))\n",
    "                for _ in range(pad_len):\n",
    "                    vec_labels.append(act2i[SILENT])\n",
    "#                 print('vec_labels', len(vec_labels))\n",
    "                labels_var.append(torch.LongTensor(vec_labels))\n",
    "            labels_var = to_var(torch.stack(labels_var, 0))\n",
    "            context = copy.deepcopy([d[2] for d in batch])\n",
    "            for i, c in enumerate(context):\n",
    "                pad_len = dialog_maxlen - len(c)\n",
    "                for _ in range(pad_len):\n",
    "                    context[i].append([1] * len(entities.keys()))\n",
    "            context = to_var(torch.FloatTensor(context))\n",
    "            pred = model(uttr_var, context)\n",
    "            action_size = pred.size(-1)\n",
    "            loss = F.nll_loss(pred.view(-1, action_size), labels_var.view(-1))\n",
    "            print('loss', loss.data[0])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "train(model, train_data, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
